{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d8e03d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c85ee5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "057dc2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5f7129f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88961299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# the unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "579bdab7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f27de4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0aef0444",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12e09c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a83a1f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f845090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "562e7419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a900c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46c89f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a847ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "60e13d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0972adba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bf89501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "38aaa22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2b20f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "66e54210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "11209cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "51b6257b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dba23e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "11379d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a64dfd2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "291e9724",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8519b67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e17217a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([60, 51, 21, 45, 60, 11,  6,  2, 17, 61,  8, 51, 31, 11, 47, 25, 62,\n",
       "       62, 37, 50, 54, 45, 32,  5, 45, 11,  2, 25, 29, 24, 21, 55, 22, 44,\n",
       "       27, 55, 18, 24, 34,  2, 62,  2, 10, 46, 57, 47, 37,  6, 62, 18, 59,\n",
       "       54, 28, 34, 62, 18, 27, 53, 50, 40, 52, 48, 64, 25,  1, 38, 43, 10,\n",
       "       29, 23, 50, 64,  1,  3, 44, 22, 35, 24, 55, 61, 24, 57, 65,  9, 17,\n",
       "       35, 29, 16, 37, 44, 10, 56, 49, 14, 35, 48, 29, 22, 53, 17],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "535b45bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b' his office lacks a helper: if\\nyou will take it on you to assist him, it shall\\nredeem you from your '\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"ulHfu:' Dv-lR:hLwwXkofS&f: LPKHpIeNpEKU w 3grhX'wEtoOUwENnkamiyL\\nYd3PJky\\n!eIVKpvKrz.DVPCXe3qjAViPInD\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ad1dea42",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fedd3e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.190439, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e076b0a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "66.051796"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5a3aed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "feb6fd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "06023d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8a70d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 425s 2s/step - loss: 2.7130\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 651s 4s/step - loss: 1.9821\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 534s 3s/step - loss: 1.7054\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 463s 3s/step - loss: 1.5465\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 607s 4s/step - loss: 1.4491\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 555s 3s/step - loss: 1.3810\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 464s 3s/step - loss: 1.3291\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 455s 3s/step - loss: 1.2849\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 376s 2s/step - loss: 1.2440\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 477s 3s/step - loss: 1.2045\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 459s 3s/step - loss: 1.1650\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 367s 2s/step - loss: 1.1237\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 366s 2s/step - loss: 1.0820\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 346s 2s/step - loss: 1.0362\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 413s 2s/step - loss: 0.9884\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 397s 2s/step - loss: 0.9380\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 365s 2s/step - loss: 0.8866\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 419s 2s/step - loss: 0.8343\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 543s 3s/step - loss: 0.7830\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 364s 2s/step - loss: 0.7329\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0aa31d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6432114a",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "17012c65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Gentle me, then 'tis, this is true.\n",
      "Do you see your repost us roat! what now is he\n",
      "to-morrow on himself, that to return from brow?\n",
      "Let her he threw demand of care?\n",
      "\n",
      "LADY CAPULET:\n",
      "O weak, of Saint Lebute And how comes your heart!\n",
      "\n",
      "PRINCE EDWARD:\n",
      "A pazier and usurp the tyrann:\n",
      "'Tis he expressly half med lock and leave.\n",
      "\n",
      "CAPULET:\n",
      "Go, see how you are sent?\n",
      "For thee, for it against my husband:\n",
      "A woman's tear, prince.\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "Holy Harry, hark, Tranio.\n",
      "\n",
      "TRANIO:\n",
      "Care he that got it is: and therefore I was passed\n",
      "Will buy his meal on pire, and seem those that\n",
      "Which we have put in your pelich grave:\n",
      "But in this hand behelding impatience\n",
      "This enemies to reason hither in France:\n",
      "Let him not let him sleep.\n",
      "\n",
      "BRUTUS:\n",
      "If love shed faieh favoury!\n",
      "Kengrates for the root of my side:\n",
      "So that, by this our pembling: no repent, measure\n",
      "Thou canst not lechery: wisely, discretitude\n",
      "Convenience suit of Exeter, here's to renison:\n",
      "We are undone! Alas, I thought, fellow!\n",
      "\n",
      "ROMEO:\n",
      "Now, but that calls for h \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 4.122150659561157\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0339cd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nSuch windows, leads for sea in very\\nTook that I should not see, pardon, alazation conceives\\nRethinking to a man that loves me grown.\\n\\nPRINCE EDWARD:\\nNo, in good night, kind lords, to writ thou wert knocks.\\n\\nVOLUMNIA:\\nHe had not froming the extremation of the time,\\nWhosevere gilthy bones and changeless and fair designs,\\nStrike thee--\\n\\nKING RICHARD III:\\nLo, to silver, mine own.\\n\\nPRINCE:\\nBe rude that make is Goutt to show his birth,\\nEngage him: something hold! how loathstains me\\nAs to be joy and enforce mischief: it was three hangman: pray,\\na beauty cannot scilf: my kinsmen did remore.\\nNow, therefore was most forewell.\\n\\nBRUKENBURY:\\nNo, master; and I'll make my kingdom for a king.\\n\\nGLOUCESTER:\\nGo you then, to myself affright! thy wid,\\nDost thou not wart thee for it. Widow him, tubstances of question,\\nIs with me all to fiss; and espect her good\\nCompare her welcome. Where stands light, foes!\\n\\nServant:\\nUp.\\n\\nROMEO:\\nNurse!\\n\\nMuss and Lidia?\\n\\nCAMILLO:\\nSir, I fear, and say thou wilt.\\n\\nWARWICK:\\nSo\"\n",
      " b\"ROMEO:\\nGood lords, hath Roman rather should.\\n\\nLORD WILLOUGHBY:\\nBecame of men, that creep like sea.\\n\\nKATHARINA:\\nA very seemer black, sir.\\nFarewell! what news?\\nHave I not hear of what they lie?\\n\\nCATESBY:\\nMistress Nevort, shall we give our noble uncounters,\\nUpon a pleasure receive the rough, I know\\nnot how to suppose and thank me: Overdong me\\nIn being so: 'tis well she told me to his wife.\\n\\nKATHARINA:\\nI care nor poisonous for a fearful spoil.\\n\\nShepherd:\\nLord Richard, that did not sain trouble you to us.\\nBut now I am safe, I will leave my kingdommass'd!\\nWelcome, good Clarence; throw up with him!\\nWhat black father well my tale of Jacket? Thou! Marcius,\\nWhom sucher joy as lies he ears carved from hand\\nMarch'd time but as the deputy?\\n\\nCAMILLO:\\nSent by the commons abroad, the own of\\nthis sister, Rivers, says that Henry his daughter;\\nBut now, a resolution! come, I pray you,\\nAnd follows to this night. Fare you well.\\n\\nDUCHESS OF YORK:\\nAgainst me, Kate, and first offend\\nYour sheep-shearing town among th\"\n",
      " b\"ROMEO:\\nThere is another father: but the overgly post\\nServive, as I have said, as it hearts, 'tis enough?\\nWho is enough? Where are my sweet house,\\nAnd welcome! Poor Clarence help to a deed who?\\n\\nRIVERS:\\nBy taste! let's far in triumph. And I for this our princely head\\nShall I be colicitch'd the offignce ta'en\\nAnd bring you for his tity with him.\\nTo whom I send to his hate by the morning?\\n\\nPRINCE EDWARD:\\nAn o'ly came more another bank.\\nCome, Kate, that I am now it too,\\nWhich cannot find the grace to you;\\nFor I have power to keep him jolugh to the ground.\\nNurse perchance of Norfolk.\\n\\nKING RICHARD II:\\nMarry, as he as prettily as the hand,\\nAs hardly benevite to the teeth? Now, my son!\\n\\nKING RICHARD III:\\nBut is grief in town, or thee?\\nFor now the strongest never spake of heaven,\\nNot sweet again: would morrow out, his wife\\nHath shown too much place dran he hath power\\nBoth raise the causer of the world:\\nAnd yet not sleep, and he but dealthing\\nA glass and liar to a pepulatic;\\nAnd there can distrain th\"\n",
      " b\"ROMEO:\\nThe rest were rise, who loves me:\\nTheir tender beauty news that devised me\\nWith desires in a holy man, along to say.\\nFor shame to have such vows alone,\\nLet Rome himself by calmmn prosperty.\\nWhat are you, sir; but, I am one to--\\nTo speak with Coriolanus: but, as thou livest is fear'd\\nTo so much interesteders of constant,\\nThoughts, thousand times made peace; and, toward'st\\nUpon respected when comes branght.\\n\\nELBOW:\\nAy, sir, it is employ'd: and why he is already?\\n\\nHASTINGS:\\nGod sir; I never dare not stumble; or at leisure\\nCoult not as he are not expir'd; but yet\\nLet us so murder: myself, our Montague of he\\nis of some power: but infirmitation base\\nAssation of my kin, aud die.\\n\\nKING RICHARD II:\\nWe do iold mischief: if it be\\nThat oath be out of mine accusation;\\nFrom what we had so, some cable\\nYour choice is not pleased for you; and preserve meet of weeping\\nHerself he were rived speech: if it up not more\\nThan do some poisonous never and\\ndispleasure's person, state report to take life\\nFrom th\"\n",
      " b\"ROMEO:\\nGood are you, sir?\\n\\nLUCIO:\\nWhat were you bite\\nIs much beheld for Romeo may born:\\nFor, hear me not?\\n\\nGREMIO:\\nNot to the Take. I knot what of him.\\n\\nBUCKINGHAM:\\nWish! Work in thy chamacheritable charge\\nThat verit enough be in his life.\\n\\nGLOUCESTER:\\nI do believe thee each, to wive heart's death, makes a grave,\\nAnd bow probbrias me you fair country.\\n\\nCLIFFORD:\\nNo, my good lord; it profession seems your kind her nature\\nDid sold that play for your chair with the\\nigncress, and importune me and\\nmore, when the help of these.\\nFor night to back that you make coat,\\nAnd I'll Halthwary replititude.\\n\\nALONSO:\\nWither.\\n\\nDORCAS:\\nWe were so not a sister, by a swaled biand\\nTo stride about thy character.\\nThose tedious as the cost!\\n\\nTHOMAS:\\nMost stayed with heavy in his looks?\\n\\nJOHN OF GAUNT:\\nNo, now I know not what I am Calable: but I cannot speak\\nJusties to the prince within the commonalty\\nof helping heart.\\n\\nFRAnce I did my liege, or else thought that he hath\\nWowards deposed; out o' the shadow come by him.\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 4.126875877380371\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2a9a8c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x000001C00466A9E0>, because it is not built.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9e9047ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "Good paunt, they have been, and pures\n",
      "Of thy unkindness poor babes;\n",
      "'Tain: I beyelf you. True, we m\n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c2ec908",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9562908f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "38e8e663",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ddaf16f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 476s 3s/step - loss: 2.7119\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c01d42bee0>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "613c020d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 2.2096\n",
      "Epoch 1 Batch 50 Loss 2.0298\n",
      "Epoch 1 Batch 100 Loss 1.9325\n",
      "Epoch 1 Batch 150 Loss 1.8682\n",
      "\n",
      "Epoch 1 Loss: 1.9870\n",
      "Time taken for 1 epoch 458.17 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 2 Batch 0 Loss 1.8414\n",
      "Epoch 2 Batch 50 Loss 1.7722\n",
      "Epoch 2 Batch 100 Loss 1.6290\n",
      "Epoch 2 Batch 150 Loss 1.6471\n",
      "\n",
      "Epoch 2 Loss: 1.7078\n",
      "Time taken for 1 epoch 413.00 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 3 Batch 0 Loss 1.6055\n",
      "Epoch 3 Batch 50 Loss 1.5910\n",
      "Epoch 3 Batch 100 Loss 1.4885\n",
      "Epoch 3 Batch 150 Loss 1.5242\n",
      "\n",
      "Epoch 3 Loss: 1.5469\n",
      "Time taken for 1 epoch 412.61 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 4 Batch 0 Loss 1.4638\n",
      "Epoch 4 Batch 50 Loss 1.4613\n",
      "Epoch 4 Batch 100 Loss 1.4032\n",
      "Epoch 4 Batch 150 Loss 1.4216\n",
      "\n",
      "Epoch 4 Loss: 1.4496\n",
      "Time taken for 1 epoch 469.90 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 5 Batch 0 Loss 1.3874\n",
      "Epoch 5 Batch 50 Loss 1.3609\n",
      "Epoch 5 Batch 100 Loss 1.3554\n",
      "Epoch 5 Batch 150 Loss 1.3990\n",
      "\n",
      "Epoch 5 Loss: 1.3814\n",
      "Time taken for 1 epoch 588.77 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 6 Batch 0 Loss 1.3042\n",
      "Epoch 6 Batch 50 Loss 1.3486\n",
      "Epoch 6 Batch 100 Loss 1.3269\n",
      "Epoch 6 Batch 150 Loss 1.2914\n",
      "\n",
      "Epoch 6 Loss: 1.3295\n",
      "Time taken for 1 epoch 471.48 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 7 Batch 0 Loss 1.2660\n",
      "Epoch 7 Batch 50 Loss 1.3205\n",
      "Epoch 7 Batch 100 Loss 1.2760\n",
      "Epoch 7 Batch 150 Loss 1.2928\n",
      "\n",
      "Epoch 7 Loss: 1.2838\n",
      "Time taken for 1 epoch 404.00 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 8 Batch 0 Loss 1.2247\n",
      "Epoch 8 Batch 50 Loss 1.2567\n",
      "Epoch 8 Batch 100 Loss 1.2367\n",
      "Epoch 8 Batch 150 Loss 1.2294\n",
      "\n",
      "Epoch 8 Loss: 1.2424\n",
      "Time taken for 1 epoch 407.01 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 9 Batch 0 Loss 1.2148\n",
      "Epoch 9 Batch 50 Loss 1.1760\n",
      "Epoch 9 Batch 100 Loss 1.2081\n",
      "Epoch 9 Batch 150 Loss 1.1943\n",
      "\n",
      "Epoch 9 Loss: 1.2021\n",
      "Time taken for 1 epoch 403.48 sec\n",
      "________________________________________________________________________________\n",
      "Epoch 10 Batch 0 Loss 1.1639\n",
      "Epoch 10 Batch 50 Loss 1.1826\n",
      "Epoch 10 Batch 100 Loss 1.1036\n",
      "Epoch 10 Batch 150 Loss 1.1849\n",
      "\n",
      "Epoch 10 Loss: 1.1612\n",
      "Time taken for 1 epoch 419.48 sec\n",
      "________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    mean.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "    print(\"_\"*80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee22628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f2846b02",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d9824ea8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
