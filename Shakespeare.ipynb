{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a53e63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07376f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "34b90a3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of text: 1115394 characters\n"
     ]
    }
   ],
   "source": [
    "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
    "print(f'Length of text: {len(text)} characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d69b17ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text[:250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab46d5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 unique characters\n"
     ]
    }
   ],
   "source": [
    "# the unique characters in the file\n",
    "vocab = sorted(set(text))\n",
    "print(f'{len(vocab)} unique characters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de5564d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_texts = ['abcdefg', 'xyz']\n",
    "\n",
    "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2ef9232",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_from_chars = tf.keras.layers.StringLookup(\n",
    "    vocabulary=list(vocab), mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "946261d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids = ids_from_chars(chars)\n",
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fcdcc75",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars_from_ids = tf.keras.layers.StringLookup(\n",
    "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e9228325",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chars = chars_from_ids(ids)\n",
    "chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b56c12bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([b'abcdefg', b'xyz'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.strings.reduce_join(chars, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "89a23b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_from_ids(ids):\n",
    "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33710a9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1], dtype=int64)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
    "all_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0c40e687",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "44c7ebdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F\n",
      "i\n",
      "r\n",
      "s\n",
      "t\n",
      " \n",
      "C\n",
      "i\n",
      "t\n",
      "i\n"
     ]
    }
   ],
   "source": [
    "for ids in ids_dataset.take(10):\n",
    "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dafaf83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_length = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e580f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
      " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
      " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
      " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
      " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
      " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
      " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
      " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
    "\n",
    "for seq in sequences.take(1):\n",
    "  print(chars_from_ids(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d868bd84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
      "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
      "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
      "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
      "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
     ]
    }
   ],
   "source": [
    "for seq in sequences.take(5):\n",
    "  print(text_from_ids(seq).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc9da91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(sequence):\n",
    "    input_text = sequence[:-1]\n",
    "    target_text = sequence[1:]\n",
    "    return input_text, target_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bee4caf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0b0f02e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
      "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
     ]
    }
   ],
   "source": [
    "for input_example, target_example in dataset.take(1):\n",
    "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
    "    print(\"Target:\", text_from_ids(target_example).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c2463fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch size\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 10000\n",
    "\n",
    "dataset = (\n",
    "    dataset\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE, drop_remainder=True)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5ea5697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of the vocabulary in StringLookup Layer\n",
    "vocab_size = len(ids_from_chars.get_vocabulary())\n",
    "\n",
    "# The embedding dimension\n",
    "embedding_dim = 256\n",
    "\n",
    "# Number of RNN units\n",
    "rnn_units = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f2810a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
    "    super().__init__(self)\n",
    "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True)\n",
    "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "  def call(self, inputs, states=None, return_state=False, training=False):\n",
    "    x = inputs\n",
    "    x = self.embedding(x, training=training)\n",
    "    if states is None:\n",
    "      states = self.gru.get_initial_state(x)\n",
    "    x, states = self.gru(x, initial_state=states, training=training)\n",
    "    x = self.dense(x, training=training)\n",
    "\n",
    "    if return_state:\n",
    "      return x, states\n",
    "    else:\n",
    "      return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "79c2b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f1ed89a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "    example_batch_predictions = model(input_example_batch)\n",
    "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "612af954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"my_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       multiple                  16896     \n",
      "                                                                 \n",
      " gru (GRU)                   multiple                  3938304   \n",
      "                                                                 \n",
      " dense (Dense)               multiple                  67650     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,022,850\n",
      "Trainable params: 4,022,850\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c7e48ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b9cd594",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([42, 32, 28, 13, 46, 64, 45, 22, 32, 10, 64, 45, 27, 28, 10, 15, 45,\n",
       "       36, 21, 42, 51, 44, 18, 41, 11, 61, 10,  8, 18, 28, 41, 12, 47, 26,\n",
       "       37,  4, 54, 27, 30,  3, 48, 28, 26, 32, 64, 10,  5,  6, 57, 13, 23,\n",
       "       43, 11, 16, 36, 29, 23, 29, 32, 44, 51, 40, 60, 35, 25, 18, 62, 33,\n",
       "       28, 27, 29, 34,  2, 51, 25, 35, 17, 26, 60, 28, 24, 13,  3, 43, 48,\n",
       "       51, 47, 56, 17, 65, 58, 39, 12, 42, 31,  7, 45, 48, 60,  9],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "be85c881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input:\n",
      " b'ur too?\\n\\nFLORIZEL:\\nAnd he, and more\\nThan he, and men, the earth, the heavens, and all:\\nThat, were I '\n",
      "\n",
      "Next Char Predictions:\n",
      " b\"cSO?gyfIS3yfNO3BfWHcleEb:v3-EOb;hMX$oNQ!iOMSy3&'r?Jd:CWPJPSelauVLEwTONPU lLVDMuOK?!dilhqDzsZ;cR,fiu.\"\n"
     ]
    }
   ],
   "source": [
    "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
    "print()\n",
    "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ee1229a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "82d4d928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
      "Mean loss:         tf.Tensor(4.1883345, shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"Mean loss:        \", example_batch_mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c83e490d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65.91292"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.exp(example_batch_mean_loss).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d4c5c6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8b1f6521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0c6335ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b8d65fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "172/172 [==============================] - 494s 3s/step - loss: 2.7083\n",
      "Epoch 2/20\n",
      "172/172 [==============================] - 515s 3s/step - loss: 1.9812\n",
      "Epoch 3/20\n",
      "172/172 [==============================] - 465s 3s/step - loss: 1.7010\n",
      "Epoch 4/20\n",
      "172/172 [==============================] - 448s 3s/step - loss: 1.5408\n",
      "Epoch 5/20\n",
      "172/172 [==============================] - 478s 3s/step - loss: 1.4455\n",
      "Epoch 6/20\n",
      "172/172 [==============================] - 486s 3s/step - loss: 1.3794\n",
      "Epoch 7/20\n",
      "172/172 [==============================] - 484s 3s/step - loss: 1.3269\n",
      "Epoch 8/20\n",
      "172/172 [==============================] - 586s 3s/step - loss: 1.2830\n",
      "Epoch 9/20\n",
      "172/172 [==============================] - 407s 2s/step - loss: 1.2416\n",
      "Epoch 10/20\n",
      "172/172 [==============================] - 317s 2s/step - loss: 1.2016\n",
      "Epoch 11/20\n",
      "172/172 [==============================] - 319s 2s/step - loss: 1.1624\n",
      "Epoch 12/20\n",
      "172/172 [==============================] - 320s 2s/step - loss: 1.1217\n",
      "Epoch 13/20\n",
      "172/172 [==============================] - 303s 2s/step - loss: 1.0784\n",
      "Epoch 14/20\n",
      "172/172 [==============================] - 304s 2s/step - loss: 1.0319\n",
      "Epoch 15/20\n",
      "172/172 [==============================] - 322s 2s/step - loss: 0.9835\n",
      "Epoch 16/20\n",
      "172/172 [==============================] - 383s 2s/step - loss: 0.9352\n",
      "Epoch 17/20\n",
      "172/172 [==============================] - 381s 2s/step - loss: 0.8830\n",
      "Epoch 18/20\n",
      "172/172 [==============================] - 374s 2s/step - loss: 0.8318\n",
      "Epoch 19/20\n",
      "172/172 [==============================] - 369s 2s/step - loss: 0.7798\n",
      "Epoch 20/20\n",
      "172/172 [==============================] - 373s 2s/step - loss: 0.7326\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2481f7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneStep(tf.keras.Model):\n",
    "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
    "    super().__init__()\n",
    "    self.temperature = temperature\n",
    "    self.model = model\n",
    "    self.chars_from_ids = chars_from_ids\n",
    "    self.ids_from_chars = ids_from_chars\n",
    "\n",
    "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
    "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
    "    sparse_mask = tf.SparseTensor(\n",
    "        # Put a -inf at each bad index.\n",
    "        values=[-float('inf')]*len(skip_ids),\n",
    "        indices=skip_ids,\n",
    "        # Match the shape to the vocabulary\n",
    "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
    "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
    "\n",
    "  @tf.function\n",
    "  def generate_one_step(self, inputs, states=None):\n",
    "    # Convert strings to token IDs.\n",
    "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
    "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
    "\n",
    "    # Run the model.\n",
    "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
    "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
    "                                          return_state=True)\n",
    "    # Only use the last prediction.\n",
    "    predicted_logits = predicted_logits[:, -1, :]\n",
    "    predicted_logits = predicted_logits/self.temperature\n",
    "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
    "    predicted_logits = predicted_logits + self.prediction_mask\n",
    "\n",
    "    # Sample the output logits to generate token IDs.\n",
    "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
    "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
    "\n",
    "    # Convert from token ids to characters\n",
    "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
    "\n",
    "    # Return the characters and model state.\n",
    "    return predicted_chars, states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ae23e620",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "251cc66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "My ene his mother, and, good sir.\n",
      "You lords and gelded to me craven at your rearing:\n",
      "In our owl--counted fair of yonder fought\n",
      "Never since your Vilaning Richard as easy\n",
      "at Dearion, let him be praised,\n",
      "We'll keep the robbers o' the day of Rome\n",
      "Endured already, mullow me at Calais\n",
      "Could not a sliphe best of thee.\n",
      "\n",
      "GLOUCESTER:\n",
      "Why, to thy mag thyself now accept, some two no purpose\n",
      "On your hands: nothing made the rest,\n",
      "Off with the tender place Buckingham, nor no squafe,\n",
      "Slaughter'd, being an quarrel secand light,\n",
      "We three themselves, of this I find\n",
      "A marvellous favour.\n",
      "Our kneel and honest men, pluck ngraw'd;\n",
      "it is misto all the senators,\n",
      "And still without for this time to that vexant\n",
      "Gromper ages, of fortune like A foit?\n",
      "\n",
      "QUEEN:\n",
      "All buh of him.\n",
      "\n",
      "Clown:\n",
      "Nat, madam.\n",
      "\n",
      "VOLUMNIA:\n",
      "Thou dost not then vain flower, or ready\n",
      "Tremble at thieves of Rembass. What say him?\n",
      "\n",
      "Pedant:\n",
      "Ay, sir, you may not shooth the prisoner.\n",
      "\n",
      "BRUTUS:\n",
      "Go entreat them home.\n",
      "Why could you remember these next, When will b \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 8.220921754837036\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b8a400df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[b\"ROMEO:\\nAy, which you are too angry.\\n\\nGRUMIO:\\nI need not live; I love him to-morrow,\\nAnd by my hands, to pledge my thumb,\\nThat name thy cousin's lightness in this good\\nLudous in protection. Padua a bank,\\nWe'll follow heaven. Thy lordship field us\\nTo call me but as yours for the people do\\nmake good upon my baughery.\\n\\nSAMPSON:\\nI' divinement as I can make a leaves and more\\nThan any makes you doubt.\\n\\nANGELO:\\nThat she's thy name; chy veast me love it,\\nAnd be not fear. And hath he done:\\nYour feemer beasts to fight with Gaunt, a glore beauty,\\nAccoming a winner, I loved these art\\nAnd pleasant father, creyt me as a mind;\\nNo bite the wity of antied aboatics\\nconvilenerously chance again: in another sen\\nShall fall, tut off the heart that means put on thus\\nwhich offer lose awain, cram's 'pord hard women's lack\\nAnd take it on thy weapon, that I may make fault.\\nGood queen, know their waters in more precious\\ncannot of your voices, lie thou the senate,\\nridges of the flower of England that will stink\\nMine hono\"\n",
      " b\"ROMEO:\\nThe worsing earth.\\n\\nLEONTES:\\nNo, why not taled so? and thou art too tain\\nto remember what thy office for a servant of this isle,\\nNere to clam the targets of the afternoon,\\nAnd then do us that word 'by to all that bear their\\neyes would say by mercy he will command.\\nWhat, did this gentleman to tell this bling for the\\n\\nThird Messenger:\\nPrepare you, then, will hee were nine to-night.\\n\\nBALTHASAR:\\nI swear at, and that I love. I'll resign him when you found.\\n\\nJULIET:\\nO, liee is good to Romeo:\\nWhere is thy peer, the man you are not swift contract:\\nNothing of that noble lord posturbing issue,\\nHe will have solicit you the Tower.\\nAnd what then?\\n\\nFirst Citizen:\\nHe shall were tears dull toiture.\\n\\nMOPSTA:\\nShe she lives in peace, I give you then.\\n\\nSecond Murderer:\\nRevenge; my brother Gloucester, billads, ladies to her\\nTo be quit o' the greatest of our seat:\\nShe was urged to pray for me: o' middle and old\\nfall'n upon our form and soldiers, and my true love's mander straight\\nAs well as I to come; for \"\n",
      " b\"ROMEO:\\nHer father's woman groans of Edward's church-drunk,\\nBe pinate, as he hot stay.\\n\\nHORTENSIO:\\nMy swiff were forgiven, young Earl of Tick,\\nWherein thou deceit unander, these graves\\nWill grace that men allays to ushope her.\\n\\nBPAUNENIUS:\\nFarewell, my noble lords, take leave our sins.\\n\\nRIVERS:\\nMy walk is banksuage, hence; but I had pity that\\nI knew within this hour.\\n\\nFRIAR JOHN:\\nBold, good sir, for a wrecking.\\n\\nEDWARD:\\nBut for my dear conguage, I befell thee with this woman,\\nTo pardon me that that fills it ages\\nSo landewick'd tongue and tell her love--\\nTo entertain these favours it exceeds,\\nWith such a few unpress'd with fearful chance,\\nWhere have I here too much mistoly. Tull not accuse\\nEdward thy Rumona, old God and stone Paulina,\\nNot us shall well beloved for all gifts,\\nTybalt with this lady lives which I do like your love.\\n\\nRIVERS:\\nHow will the court is that affairs?\\nA accuse and said 'Twere for our foul debt. What, cause,\\nLeft on that shame! it princhss me guilty\\nTounte as go with me: I\"\n",
      " b\"ROMEO:\\nHenceforth I would, away, to thee whiph God--\\nI'll in to him and thrivet.\\n\\nThird Ressen:\\nAnd I this wind, the king's right none, I take my death\\nTill Cloudd of me, Eyball'd, uncle, let me king,\\nWhen I shall laid, I like it up from my kingdom's pack,\\nWhom, then the irence the other lose your honour,\\nShalt thou see your manners.\\n\\nCAMILLO:\\nMy best orible blows upon my life!\\n\\nJOHN OF GAUNT:\\nI make; but she from whence at me, and prevail,\\nIt shall be married to counterfeition,\\nHave I offended in us all towns the more.\\n\\nBUCKINGHAM:\\nHow dot the temination? O, he cannot live.\\nWhy then, though with all Bohemia, or so?\\nYou logger-headed dester.\\n\\nPETRUCHIO:\\nWhy, let me see thee mercy:\\nWoe do not what become many foundations and\\nThan Brook, that thought thou think'st thou livest;\\nTo use the whole legding: if I have need of me.\\n\\nQUEEN MARGARET:\\nO fool! and have I heard of me\\nThat brings me see thy parention of fully down,\\nHis arms against thy pied like lips?\\n\\nFirst Senator:\\nTake mine own after?\\n\\nP\"\n",
      " b\"ROMEO:\\nStay; but her Someo slain, it shall be dead.\\nYou are lose no less than he were all.\\n\\nKING RICHARD III:\\nUther me, I pray thee, then speak.\\nNow, ere deliver me so, I say, knock as to thy heart\\nTo make reckony. All sense, being run,\\nAn handred by the idle blows, make for his estate.\\n\\nQUEEN ELIZABETH:\\nIt is too chide the malinence of his office,\\nWhich have full, thy song encomforted with his heart;\\nWhereto we will bear, and let him there want no sign?\\nHave I not heed in Padua had been proud; and bring it rule\\nwonders do prosert by and baggs in't.\\n\\nCORIOLANUS:\\nO thought, sir, haply my presence but nothing\\nTun thee to the king in so fair.\\n\\nFRIAR LAURENCE:\\nGod for my life, sir, Grey, chat with his body\\nAnd hither cut thy children in them hot with\\nAny full of pilots, for lace is come to know\\nWhat wares have said for Various, that is so?\\n\\nDUKE OF YORK:\\nHe swore, no boot.\\n\\nANGELO:\\nWell; what's your cousin's death?\\n\\nSecond Citizen:\\nYou have been thy soul's consent to your chamber\\nInter the extra\"], shape=(5,), dtype=string) \n",
      "\n",
      "________________________________________________________________________________\n",
      "\n",
      "Run time: 8.323888778686523\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "states = None\n",
    "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(1000):\n",
    "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "result = tf.strings.join(result)\n",
    "end = time.time()\n",
    "print(result, '\\n\\n' + '_'*80)\n",
    "print('\\nRun time:', end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f60ebb6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x00000187C951A9B0>, because it is not built.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
      "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _update_step_xla, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: one_step\\assets\n"
     ]
    }
   ],
   "source": [
    "tf.saved_model.save(one_step_model, 'one_step')\n",
    "one_step_reloaded = tf.saved_model.load('one_step')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "63ae35c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROMEO:\n",
      "The motion's kingly gives ready off.\n",
      "\n",
      "ROMEO:\n",
      "A lamb indaged for his that: more accuser,\n",
      "This other \n"
     ]
    }
   ],
   "source": [
    "states = None\n",
    "next_char = tf.constant(['ROMEO:'])\n",
    "result = [next_char]\n",
    "\n",
    "for n in range(100):\n",
    "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
    "  result.append(next_char)\n",
    "\n",
    "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d812d5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTraining(MyModel):\n",
    "  @tf.function\n",
    "  def train_step(self, inputs):\n",
    "      inputs, labels = inputs\n",
    "      with tf.GradientTape() as tape:\n",
    "          predictions = self(inputs, training=True)\n",
    "          loss = self.loss(labels, predictions)\n",
    "      grads = tape.gradient(loss, model.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "      return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "027cf81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomTraining(\n",
    "    vocab_size=len(ids_from_chars.get_vocabulary()),\n",
    "    embedding_dim=embedding_dim,\n",
    "    rnn_units=rnn_units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a9c27fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4b3a4ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172/172 [==============================] - 521s 3s/step - loss: 2.7032\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x187d903be50>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df5a99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "mean = tf.metrics.Mean()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "\n",
    "    mean.reset_states()\n",
    "    for (batch_n, (inp, target)) in enumerate(dataset):\n",
    "        logs = model.train_step([inp, target])\n",
    "        mean.update_state(logs['loss'])\n",
    "\n",
    "        if batch_n % 50 == 0:\n",
    "            template = f\"Epoch {epoch+1} Batch {batch_n} Loss {logs['loss']:.4f}\"\n",
    "            print(template)\n",
    "\n",
    "    # saving (checkpoint) the model every 5 epochs\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        model.save_weights(checkpoint_prefix.format(epoch=epoch))\n",
    "\n",
    "    print()\n",
    "    print(f'Epoch {epoch+1} Loss: {mean.result().numpy():.4f}')\n",
    "    print(f'Time taken for 1 epoch {time.time() - start:.2f} sec')\n",
    "    print(\"_\"*80)\n",
    "\n",
    "model.save_weights(checkpoint_prefix.format(epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21457644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
